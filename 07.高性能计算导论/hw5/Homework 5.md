# Homework 5

2018011365 张鹤潇

```shell
# 以4进程, 每个进程2线程运行PA, 矩阵规模7200*7200
srun -n 4 PA 7200 2 
# 以10线程运行PA5.3, 向量长度1e8
srun PA5.3 100000000 10
```

## Ex 5.4

初始化为对应运算的单位元，如下表。

| 操作符 | 初始值                                                       |
| ------ | ------------------------------------------------------------ |
| &&     | 1                                                            |
| \|\|   | 0                                                            |
| &      | $\begin{matrix} \underbrace{1\cdots 1} \\ \text{bits of type} \end{matrix}$ |
| \|     | 0                                                            |
| ^      | 0                                                            |

## Ex 5.5

(a). 串行执行的结果为;
$$
2.0+2.0+4.0+1000.0\approx1010.
$$
(b). 并行执行的结果为:
$$
(2.0+2.0)+(4.0+1000.0)\approx4.+1000.\approx1000.
$$

## Ex PA

### 解题思路

我沿用了第三次作业PA3.6中的MPI程序框架，即将整个向量广播给所有进程，让进程自己决定需要哪一部分向量进行运算，并将结果保存到`local_result`的相应位置，最后归约得到结果。与按行列分组两次派发相比，这种做法在实现上更简单，虽然略微增加计算时间，但缩短了数据分发的时间，总效率更高。在矩阵乘法和二范数计算模块调用OpenMP.

### 代码实现

```c++
// 矩阵运算模块
#pragma omp parallel for num_threads(thread_cnt) schedule(static, 4)
    for (int i = 0; i < loc_n; i++) {
        double tmp = 0;
        for (int j = 0; j < loc_n; j++) {
            tmp += loc_mat[i * loc_n + j] * vector[vec_off + j];
        }
        loc_res[res_off + i] = tmp;
    }
// ...
// 二范数计算模块
#pragma omp parallel for num_threads(thread_cnt)
        for (int i = 0; i < n; i++) {
            norm_diff += powl(ser_res[i] - res[i], 2.);
        }
        norm_diff = sqrtl(norm_diff);
```

### 测试结果

串行程序与并行程序结果之差的二范数为0，证明了程序的正确性。

矩阵乘法部分测试结果如下，调度策略为`static, 4`.  注意，图表中`th=1`表示的是第三次作业中编写的程序。

线程数$p=1$时, 

| th   | n     | 数据分发时间 (s) | 计算时间 (s) | 总时间 (s) | 加速比(+数据分发) | 并行效率(+) | 加速比(-数据分发) | 并行效率(-) |
| ---- | ----- | ---------------- | ------------ | ---------- | ----------------- | ----------- | ----------------- | ----------- |
| 1    | 3600  | 0.032            | 0.009        | 0.041      | 0.21              | 0.21        | 1.00              | 1.00        |
| 1    | 7200  | 0.124            | 0.033        | 0.157      | 0.21              | 0.21        | 1.00              | 1.00        |
| 1    | 14400 | 0.496            | 0.135        | 0.632      | 0.21              | 0.21        | 1.00              | 1.00        |
| 2    | 3600  | 0.033            | 0.01         | 0.043      | 0.20              | 0.10        | 0.81              | 0.41        |
| 2    | 7200  | 0.126            | 0.032        | 0.158      | 0.22              | 0.11        | 1.07              | 0.54        |
| 2    | 14400 | 0.506            | 0.113        | 0.619      | 0.23              | 0.12        | 1.27              | 0.64        |
| 4    | 3600  | 0.032            | 0.007        | 0.039      | 0.22              | 0.06        | 1.21              | 0.30        |
| 4    | 7200  | 0.124            | 0.02         | 0.144      | 0.23              | 0.06        | 1.68              | 0.42        |
| 4    | 14400 | 0.501            | 0.074        | 0.575      | 0.25              | 0.06        | 1.97              | 0.49        |
| 6    | 3600  | 0.033            | 0.007        | 0.039      | 0.22              | 0.04        | 1.30              | 0.22        |
| 6    | 7200  | 0.124            | 0.019        | 0.143      | 0.24              | 0.04        | 1.77              | 0.30        |
| 6    | 14400 | 0.494            | 0.061        | 0.556      | 0.28              | 0.05        | 2.50              | 0.42        |
| 8    | 3600  | 0.033            | 0.008        | 0.04       | 0.21              | 0.03        | 1.14              | 0.14        |
| 8    | 7200  | 0.126            | 0.017        | 0.143      | 0.24              | 0.03        | 1.95              | 0.24        |
| 8    | 14400 | 0.494            | 0.058        | 0.553      | 0.26              | 0.03        | 2.47              | 0.31        |

![1587301734724](.\pic\3.png)

![1587301549714](.\pic\4.png)

$p=4$时，

| th   | n     | 数据分发时间 (s) | 计算时间 (s) | 总时间 (s) | 加速比(+数据分发) | 并行效率 | 加速比(-数据分发) | 并行效率 |
| ---- | ----- | ---------------- | ------------ | ---------- | ----------------- | -------- | ----------------- | -------- |
| 1    | 3600  | 0.046            | 0.002        | 0.049      | 0.17              | 0.043    | 3.51              | 0.878    |
| 1    | 7200  | 0.21             | 0.009        | 0.219      | 0.15              | 0.038    | 3.65              | 0.913    |
| 1    | 14400 | 0.797            | 0.039        | 0.835      | 0.17              | 0.043    | 3.69              | 0.923    |
| 2    | 3600  | 0.045            | 0.026        | 0.07       | 0.12              | 0.015    | 0.34              | 0.043    |
| 2    | 7200  | 0.204            | 0.039        | 0.243      | 0.14              | 0.018    | 0.89              | 0.111    |
| 2    | 14400 | 0.799            | 0.065        | 0.865      | 0.17              | 0.021    | 2.19              | 0.274    |
| 4    | 3600  | 0.045            | 0.006        | 0.051      | 0.17              | 0.011    | 1.36              | 0.085    |
| 4    | 7200  | 0.203            | 0.021        | 0.224      | 0.15              | 0.009    | 1.63              | 0.102    |
| 4    | 14400 | 0.803            | 0.056        | 0.859      | 0.17              | 0.011    | 2.57              | 0.161    |
| 6    | 3600  | 0.045            | 0.007        | 0.052      | 0.17              | 0.007    | 1.23              | 0.051    |
| 6    | 7200  | 0.202            | 0.016        | 0.218      | 0.15              | 0.006    | 2.08              | 0.087    |
| 6    | 14400 | 0.847            | 0.055        | 0.902      | 0.16              | 0.007    | 2.70              | 0.113    |
| 8    | 3600  | 0.044            | 0.072        | 0.116      | 0.07              | 0.002    | 0.12              | 0.004    |
| 8    | 7200  | 0.214            | 0.017        | 0.231      | 0.14              | 0.004    | 1.99              | 0.062    |
| 8    | 14400 | 0.834            | 0.063        | 0.897      | 0.15              | 0.005    | 2.23              | 0.070    |

![1587302540972](.\pic\5.png)

![1587302550394](.\pic\6.png)

$p=9$时，

| th   | n     | 数据分发时间 (s) | 计算时间 (s) | 总时间 (s) | 加速比(+数据分发) | 并行效率 | 加速比(-数据分发) | 并行效率 |
| ---- | ----- | ---------------- | ------------ | ---------- | ----------------- | -------- | ----------------- | -------- |
| 1    | 3600  | 0.049            | 0.002        | 0.245      | 0.17              | 0.019    | 4.04              | 0.449    |
| 1    | 7200  | 0.238            | 0.008        | 0.245      | 0.15              | 0.017    | 4.75              | 0.528    |
| 1    | 14400 | 0.841            | 0.027        | 0.868      | 0.16              | 0.018    | 4.93              | 0.548    |
| 2    | 3600  | 0.051            | 0.023        | 0.075      | 0.11              | 0.006    | 0.37              | 0.021    |
| 2    | 7200  | 0.216            | 0.051        | 0.267      | 0.13              | 0.007    | 0.67              | 0.037    |
| 2    | 14400 | 0.846            | 0.06         | 0.906      | 0.15              | 0.008    | 2.31              | 0.128    |
| 4    | 3600  | 0.049            | 0.038        | 0.087      | 0.1               | 0.003    | 0.23              | 0.006    |
| 4    | 7200  | 0.214            | 0.043        | 0.257      | 0.13              | 0.004    | 0.79              | 0.022    |
| 4    | 14400 | 0.861            | 0.076        | 0.936      | 0.15              | 0.004    | 1.83              | 0.051    |
| 6    | 3600  | 0.051            | 0.042        | 0.092      | 0.09              | 0.002    | 0.2               | 0.004    |
| 6    | 7200  | 0.222            | 0.058        | 0.28       | 0.12              | 0.002    | 0.57              | 0.011    |
| 6    | 14400 | 0.845            | 0.094        | 0.94       | 0.15              | 0.003    | 1.47              | 0.027    |
| 8    | 3600  | 0.05             | 0.078        | 0.128      | 0.07              | 0.001    | 0.11              | 0.002    |
| 8    | 7200  | 0.217            | 0.094        | 0.311      | 0.11              | 0.002    | 0.37              | 0.005    |
| 8    | 14400 | 0.855            | 0.115        | 0.97       | 0.14              | 0.002    | 1.22              | 0.017    |

![1587303264520](.\pic\7.png)

![1587303234460](.\pic\8.png)

可以看到，OpenMP的引入对数据分发时间基本没有影响，这是符合预期的；但它也没有起到预期的加速效果，甚至让程序的运行效率变低了。进程数越大，这种损害越明显。

对于二范数计算模块，在矩阵规模$n=14400$下，测试如下。

| th   | 计算时间(s) | 并行加速比 | 并行效率 |
| ---- | ----------- | ---------- | -------- |
| 1    | 0.000358    | 1.000      | 1.000    |
| 2    | 0.000219    | 1.635      | 0.817    |
| 4    | 0.000205    | 1.746      | 0.437    |
| 8    | 0.000131    | 2.733      | 0.342    |
| 16   | 0.000127    | 2.819      | 0.176    |

如此小的计算量，并行的意义并不大。程序的运行时间主要花费在矩阵运算上。

## Ex PA5.3

### 1.

私有变量: `i`, `j`, `count`; 共享变量: `a`,  `n`,  `temp`.

### 2.

不存在循环依赖，循环体中，共享变量`a,n`是只读的，`temp`是只写的，且算法保证了不同线程写入`temp`数组的位置不同。

### 3.

可以，将数组分段进行`memcpy`，但考虑到`memcpy`的效率已经很高，且程序的运行时间主要花费在$O(n^2)$的排序上，这样做的加速效果几乎可以忽略不计。

### 4.

根据题目的约定，待排序数组中所有元素都是$0\sim k-1(k=50)$间的整数。我先实现了计数排序的$O(n+k)$串行版本，只需遍历整个数组两次。

```c++
void countSort(u8* arr, const int& n) {
	int counts[limit] = {};
	for (int i = 0; i < n; i++) {
		counts[arr[i]]++;
	}
	int r = 0;
	for (int v = 0; v < limit; v++) {
		for (int i = 0; i < counts[v]; i++) {
			arr[r++] = v;
		}
	}
}
```

将上述程序按照以下步骤并行：

- 计数
  - 划分输入数组；
  - 并行统计元素出现的次数；
  - 将统计结果归约到主线程中.
- 构建排序后的新数组
  - 计算每个元素在新数组中的偏移量；
  - 按元素并行构建新数组.

```c++
void parCountSort(u8* arr, int n, int thread_cnt) {
	int counts[limit] = {}, offset[limit + 1] = {};
	int quotient = n / thread_cnt,
		remainder = n % thread_cnt;

#pragma omp parallel num_threads(thread_cnt)
	{
		int local_cnt[limit] = {};
		int tid = omp_get_thread_num();
		int beg, end;
		if (tid < remainder) {
			beg = tid * (quotient + 1);
			end = beg + (quotient + 1);
		}
		else {
			beg = tid * quotient + remainder;
			end = beg + quotient;
		}

		for (int i = beg; i < end; i++) {
			local_cnt[arr[i]]++;
		}
		for (int i = 0; i < limit; i++) {
#pragma omp atomic
			counts[i] += local_cnt[i];
		}
	}

	for (int i = 1; i <= limit; i++) {
		offset[i] = offset[i - 1] + counts[i - 1];
	}
#pragma omp parallel for num_threads(thread_cnt) schedule(dynamic, 1)
	for (int v = 0; v < limit; v++) {
		for (int i = offset[v]; i < offset[v + 1]; i++) {
			arr[i] = v;
		}
	}
}
```

其中计算偏移量的循环只执行$k$次，几乎是$O(1)$的，不需要并行。

### 5.

以`dynamic`调度策略，`chunksize=1`，在集群上测试如下。

| n    | conut sort(serial)(s) | conut sort(th=4)(s) | conut sort(th=10)(s) | qsort(s) |
| ---- | --------------------- | ------------------- | -------------------- | -------- |
| 1e7  | 0.0060                | 0.0072              | 0.0035               | 1.5383   |
| 3e7  | 0.0185                | 0.0206              | 0.0099               | 4.8537   |
| 5e7  | 0.0323                | 0.0328              | 0.0159               | 8.2038   |
| 1e8  | 0.0649                | 0.0592              | 0.0300               | 16.8981  |

可见，相比于$O(nlogn)$的`qsort`,  线性时间复杂度的计数排序优势还是很明显的。

设定不同线程数，单独测试计数排序(单位s).

| n      | 2e8    | 4e8    | 6e8    | 8e8    |
| ------ | ------ | ------ | ------ | ------ |
| serial | 0.1458 | 0.2918 | 0.4378 | 0.5829 |
| th=2   | 0.1898 | 0.3180 | 0.4271 | 0.5487 |
| th=4   | 0.1100 | 0.1898 | 0.2495 | 0.3139 |
| th=6   | 0.0809 | 0.1407 | 0.1899 | 0.2366 |
| th=8   | 0.0640 | 0.1186 | 0.1558 | 0.1906 |
| th=10  | 0.0542 | 0.0947 | 0.1426 | 0.1749 |
| th=15  | 0.0407 | 0.0745 | 0.1045 | 0.1273 |
| th=20  | 0.0327 | 0.0592 | 0.0823 | 0.1025 |

随着数据规模的增大，多线程的收益逐渐超过其额外开销，并行加速比和并行效率提高。

![1587285520620](.\pic\1.png)

![1587285602824](.\pic\2.png)

### 6.

在n=5e8下，对于不同的调度策略和chunksize，分别在线程数为8(不整除$k$)和10(整除k)下测试(单位s)。

线程数8:

| chunksize | static     | dynamic    | guided     |
| --------- | ---------- | ---------- | ---------- |
| 1         | **0.1493** | **0.1398** | 0.1508     |
| 2         | 0.1596     | 0.1451     | 0.1479     |
| 4         | 0.1577     | 0.1469     | **0.1478** |
| 6         | 0.1549     | 0.1465     | 0.1498     |
| 8         | 0.1591     | 0.1484     | 0.1569     |
| 10        | 0.1716     | 0.1630     | 0.1717     |
| 15        | 0.2044     | 0.2120     | 0.1969     |
| 20        | 0.2434     | 0.2195     | 0.2330     |
| 25        | 0.2708     | 0.2720     | 0.2543     |

线程数10:

| chunksize | static     | dynamic    | guided     |
| --------- | ---------- | ---------- | ---------- |
| 1         | **0.1158** | **0.1185** | **0.1183** |
| 2         | 0.1248     | 0.1225     | 0.1217     |
| 4         | 0.1450     | 0.1382     | 0.1392     |
| 6         | 0.1273     | 0.1286     | 0.1275     |
| 8         | 0.1453     | 0.1415     | 0.1454     |
| 10        | 0.1645     | 0.1619     | 0.1617     |
| 15        | 0.1929     | 0.1897     | 0.1896     |
| 20        | 0.2383     | 0.2233     | 0.2269     |
| 25        | 0.2545     | 0.2471     | 0.2528     |

可见，三种调度方法的效率差异不大，`static`和`dynamic`在chunksize=1时效率最高。这是因为数组中的元素大致满足$0\sim k-1$间的均匀分布，每轮外循环的循环次数基本相同，且都在一千万（$\frac{5\times 10^8}{50}$）左右。

`dynamic,1`是一种较合理的调度策略。