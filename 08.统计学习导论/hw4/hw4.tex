%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do not alter this block (unless you're familiar with LaTeX
\documentclass[UTF8,12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, fancyhdr, color, comment, graphicx, environ}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage{xeCJK} %导入中文包
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue,
}

\pagestyle{fancy}

\newenvironment{problem}[2][Problem]
{ \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2}}
	{  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
{\textit{Proof:}}
{}
\newenvironment{answer}
{}
{}
\newenvironment{eq}
{
	\begin{equation}
		\begin{aligned}\nonumber
}
{
		\end{aligned}
	\end{equation}
}

% prevent line break in inline mode
\binoppenalty=\maxdimen
\relpenalty=\maxdimen

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Fill in the appropriate information below
\lhead{张鹤潇 2018011365}
\rhead{Statistical Learning} 
\chead{\textbf{Homework 4}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\renewcommand{\qed}{\quad\qedsymbol}
\begin{problem}{Ex.1}
\end{problem}
\begin{answer}
\begin{eq}
	\hat{y}(x)&=\mathop{argmax}\limits_{k} P(Y=k|x)\\
	&= \mathop{argmax}\limits_{k} f_k(x)\pi_k\\
	&= \mathop{argmax}\limits_{k} \log{f_k(x)}+\log{\pi_k}\\
	\log{f_k(x)}&=-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)
	-\frac{p}{2}\log{2\pi}-\frac{1}{2}\log{|\Sigma_k|}\\
	&= x^T\Sigma^{-1}_k\mu_k - \frac{1}{2}\mu_k^T \Sigma_k^{-1}\mu_k
	-\frac{1}{2}(\log{|\Sigma_k|}+p\log{2\pi}+x^T\Sigma^{-1}_k x)
\end{eq}
Noticed that $\Sigma_k$ is constant for each $k$ in LDA, so
\begin{eq}
	\hat{y}(x)&=\mathop{argmax}\limits_{k} (\log{\pi_k} +
	x^T\Sigma^{-1}_k\mu_k - \frac{1}{2}\mu_k^T \Sigma_k^{-1}\mu_k)
\end{eq}
\end{answer}

\begin{problem}{Ex.2}
\end{problem}
\begin{answer}
	(1).LDA classifies to class 2 if and only if $\log{\frac{f_2(x)\pi_2}{f_1(x)\pi_1}}>0$.
	\begin{eq}
		\log{\frac{f_2(x)\pi_2}{f_1(x)\pi_1}}
		&= \log{\frac{N_2}{N_1}} + x^T \Sigma^{-1} (\hat{\mu_2}-\hat{\mu_1})- 
		\frac{1}{2}(\hat{\mu_2}^T \Sigma^{-1} \hat{\mu_2} -\hat{\mu_1}^T \Sigma^{-1} \hat{\mu_1})\\
		&= \log{\frac{N_2}{N_1}} + x^T \Sigma^{-1} (\hat{\mu_2}-\hat{\mu_1}) -
		\frac{1}{2}(\hat{\mu_2} + \hat{\mu_1})^T \Sigma^{-1}(\hat{\mu_2} - \hat{\mu_1})
	\end{eq}
	So the LDA rules classifies $x$ to class 2 when
	\begin{eq}
		x^T \Sigma^{-1} (\hat{\mu_2}-\hat{\mu_1}) > 
		\frac{1}{2}(\hat{\mu_2} + \hat{\mu_1})^T \Sigma^{-1}(\hat{\mu_2} - \hat{\mu_1}) - \log{{N_2}/{N_1}}
	\end{eq}
	(2).In this particular case, 
	\begin{eq}
		\nabla_{\beta_0} &= -2\sum_{i=1}^N (y_i-\beta_0-x_i^T\beta)=0\\
		\beta_0 &= \frac{1}{N}\sum_{i=1}^N (y_i-x^T_i\beta)\\
		\nabla_{\beta}&= -2\sum_{i=1}^N (y_i-\beta_0-x_i^T\beta)x_i=0\\
		\beta_0 \sum_{i=1}^N x_i &+ \beta^T \sum_{i=1}^N x_i x_i^T = \sum_{i=1}^N y_ix_i
	\end{eq}
	Noticed that,
	\begin{eq}
		\sum_{i=1}^N y_i=&-N_1\frac{N}{N_1}+N_2\frac{N}{N_2}=0\\
		(\sum_{i=1}^N x^T_i)\beta &= \beta^T(N_1\hat{\mu_1}+N_2\hat{\mu_2})
	\end{eq}
	So $\beta_0=-\frac{1}{N}\beta^T(N_1\hat{\mu_1}+N_2\hat{\mu_2}) $. At one hand,
	\begin{eq}
		\sum_{i=1}^N y_ix_i &= -\frac{N}{N_1}N_1 \hat{\mu_1} +\frac{N}{N_2}N_2 \hat{\mu_2}
		= N(\hat{\mu_2}-\hat{\mu_1})\\
		\sum_{i=1}^N \beta_0 x_i &= -\frac{1}{N}\beta^T 
		(N_1\hat{\mu_1}+N_2\hat{\mu_2})(N_1\hat{\mu_1}+N_2\hat{\mu_2})^T\\
		&= -\frac{1}{N}\beta^T(N_1^2 \hat{\mu_1}\hat{\mu_1}^T + N_2^2 \hat{\mu_2}\hat{\mu_2}^T
		+N_1 N_2 \hat{\mu_1}\hat{\mu_2}^T+N_1 N_2 \hat{\mu_2}\hat{\mu_1}^T)
	\end{eq}
	So we got that:
	\begin{equation}
		\begin{aligned}\nonumber
			\beta_0 \sum_{i=1}^N x_i + \beta^T \sum_{i=1}^N x_i x_i^T 
		&= (\sum_{i=1}^N x_i x_i^T-\frac{N_1^2}{N} \hat{\mu_1}\hat{\mu_1}^T - \frac{N_2^2}{N} \hat{\mu_2}\hat{\mu_2}^T
		-\frac{N_1 N_2}{N} \hat{\mu_1}\hat{\mu_2}^T+ \frac{N_1 N_2}{N} \hat{\mu_2}\hat{\mu_1}^T)\beta
		\end{aligned}
	\end{equation}
	At the other hand,
	\begin{eq}
		(N-2)\Sigma&=\sum_{x\in C_1} (x-\hat{\mu_1})(x-\hat{\mu_1})^T + \sum_{x\in C_2} (x-\hat{\mu_2})(x-\hat{\mu_2})^T\\
		&= \sum_{i=1}^N x_i x_i^T - N_1 \hat{\mu_1}\hat{\mu_1}^T -N_2 \hat{\mu_2}\hat{\mu_2}^T\\
		N\Sigma_B &= \frac{N_1 N_2}{N}(\hat{\mu_1}-\hat{\mu_2})(\hat{\mu_1}-\hat{\mu_2})^T\\
		&= \frac{N_1 N_2}{N}(\hat{\mu_1}\hat{\mu_1}^T + \hat{\mu_2}\hat{\mu_2}^T - \hat{\mu_1}\hat{\mu_2}^T- \hat{\mu_2}\hat{\mu_1}^T)
	\end{eq}
	Noticed that $N=N_1+N_2$, now we got:
	\begin{eq}
		(N-2)\Sigma + N\Sigma_B&=
		\sum_{i=1}^N x_i x_i^T -\frac{N_1^2}{N} \hat{\mu_1}\hat{\mu_1}^T - \frac{N_2^2}{N} \hat{\mu_2}\hat{\mu_2}^T
		-\frac{N_1 N_2}{N} \hat{\mu_1}\hat{\mu_2}^T+ \frac{N_1 N_2}{N} \hat{\mu_2}\hat{\mu_1}^T\\
		(N-2)\Sigma+N\Sigma_B&=\beta_0 \sum_{i=1}^N x_i 
		+ \beta^T \sum_{i=1}^N x_i x_i^T 
	\end{eq}
	So,
	\begin{equation}
		\begin{aligned}
		((N-2)\Sigma+N\Sigma_B)\beta = N(\hat{\mu_2}-\hat{\mu_1})
		\end{aligned}
	\end{equation}
	(3).Noticed that $(\hat{\mu_1}-\hat{\mu_2})^T \beta\in \mathbb{R}$.
	\begin{eq}
		\Sigma_B\beta = \frac{N_1 N_2}{N^2}(\hat{\mu_1}-\hat{\mu_2})
		[(\hat{\mu_1}-\hat{\mu_2})^T \beta]
	\end{eq}
	So $\Sigma_B\beta$ is in the direction of $\hat{\mu_1}-\hat{\mu_2}$. From (1), we conclude that 
	$\Sigma\beta \propto(\hat{\mu_2}-\hat{\mu_1})$.
\end{answer}

\begin{problem}{Ex.3}
\end{problem}
\begin{answer}
	Let A denotes that the patient has flu, B denotes he/she has a fever and C denotes cough.\\
	Considering the circumstance, there should be $B \subset A$ and $C \subset A$.
\begin{eq}
	P(B|A)&=\frac{P(B)}{P(A)}=0.5\\
	P(BC|A)&=\frac{P(BC)}{P(A)}=0.25\\
	P(B)&=P(C)=0.25,P(BC)=0.125\\
	Corr(B,C)&=\frac{P(BC)-P(B)P(C)}{\sqrt{P(B)P(\bar{B})P(C)P(\bar{C})}}\\
	&= \frac{1}{3}
\end{eq}

\end{answer}
\end{document}